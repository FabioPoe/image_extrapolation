{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i will try saving everything in one file such that i dont have to open files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from architectures import *\n",
    "from dataset_numpy import *\n",
    "from utils import *\n",
    "from torch.utils.data import Subset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import json\n",
    "import h5py\n",
    "from icecream import ic\n",
    "import numpy as np\n",
    "\n",
    "#getting the configurations from the config file\n",
    "with open('working_config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "    \n",
    "testset = RandomSeqDataset(r\"D:\\studium_tests\\small_original_dataset.npz\",config[\"testset_key\"])\n",
    "trainset = RandomSeqDataset(r\"D:\\studium_tests\\small_original_dataset.npz\",config[\"trainset_key\"])\n",
    "\n",
    "\n",
    "# Create dataloaders from each subset\n",
    "test_loader = DataLoader(testset,  # we want to load our dataset\n",
    "                         shuffle=False,  # shuffle for training\n",
    "                         batch_size=1,  # 1 sample at a time\n",
    "                         num_workers=0,\n",
    "                         collate_fn=stack_if_possible_collate_fn  # no background workers\n",
    "                         )\n",
    "\n",
    "training_loader = DataLoader(trainset,  # we want to load our dataset\n",
    "                             shuffle=False,  # shuffle for training\n",
    "                             batch_size=4,  # stack 4 samples to a minibatch\n",
    "                             num_workers=0,\n",
    "                             collate_fn=stack_if_possible_collate_fn  # 2 background workers\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 90, 90])\n"
     ]
    }
   ],
   "source": [
    "for data in training_loader:\n",
    "    print(data[0].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "executionInfo": {
     "elapsed": 6290,
     "status": "ok",
     "timestamp": 1629797165470,
     "user": {
      "displayName": "Fabio PÃ¶schko",
      "photoUrl": "",
      "userId": "12121821188507889824"
     },
     "user_tz": -120
    },
    "id": "qWoAsn4MHQyk"
   },
   "source": [
    "from architectures import *\n",
    "from dataset_fast import *\n",
    "from utils import *\n",
    "from torch.utils.data import Subset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import json\n",
    "import h5py\n",
    "from icecream import ic\n",
    "\n",
    "#getting the configurations from the config file\n",
    "with open('working_config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "    \n",
    "testset = RandomSeqDataset(config[\"dataset_path\"],config[\"testset_key\"])\n",
    "trainset = RandomSeqDataset(config[\"dataset_path\"],config[\"trainset_key\"])\n",
    "\n",
    "\n",
    "# Create dataloaders from each subset\n",
    "test_loader = DataLoader(testset,  # we want to load our dataset\n",
    "                         shuffle=False,  # shuffle for training\n",
    "                         batch_size=1,  # 1 sample at a time\n",
    "                         num_workers=0,\n",
    "                         collate_fn=stack_if_possible_collate_fn  # no background workers\n",
    "                         )\n",
    "\n",
    "training_loader = DataLoader(trainset,  # we want to load our dataset\n",
    "                             shuffle=False,  # shuffle for training\n",
    "                             batch_size=4,  # stack 4 samples to a minibatch\n",
    "                             num_workers=0,\n",
    "                             collate_fn=stack_if_possible_collate_fn  # 2 background workers\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AE_CNN(nn.Module):\n",
    "    def __init__(self, ae_input_dim = 90*90, ae_middle_dim = 80*80, ae_hidden_dim = 70*70,\n",
    "                n_input_channels = 1, n_output_channels = 1, conv_kernel_size = 5, n_hidden_layers=5, n_hidden_kernels=32):\n",
    "        super(AE_CNN, self).__init__()\n",
    "        # encoder\n",
    "        #self.encoder_linear1 = nn.Linear(ae_input_dim, ae_input_dim)\n",
    "        #self.encoder_linear2 = nn.Linear(ae_input_dim, ae_input_dim)\n",
    "        \n",
    "        # convolution\n",
    "        hidden_layers = []\n",
    "        for _ in range(n_hidden_layers):\n",
    "            # Add a CNN layer\n",
    "            layer = nn.Conv2d(in_channels=n_input_channels,\n",
    "                              out_channels=n_hidden_kernels,\n",
    "                              kernel_size=conv_kernel_size)\n",
    "            hidden_layers.append(layer)\n",
    "            # Add relu activation module to list of modules\n",
    "            hidden_layers.append(nn.ReLU())\n",
    "            n_input_channels = n_hidden_kernels\n",
    "\n",
    "        # deconvolution\n",
    "        for _ in range(n_hidden_layers + 1):\n",
    "            # Add a CNN layer\n",
    "            layer = nn.ConvTranspose2d(in_channels=n_input_channels,\n",
    "                                       out_channels=n_hidden_kernels,\n",
    "                                       kernel_size=conv_kernel_size)\n",
    "            hidden_layers.append(layer)\n",
    "            # Add relu activation module to list of modules\n",
    "            hidden_layers.append(nn.ReLU())\n",
    "            n_input_channels = n_hidden_kernels\n",
    "\n",
    "            \n",
    "        self.hidden_layers = nn.Sequential(*hidden_layers)\n",
    "\n",
    "        self.output_layer = nn.Conv2d(in_channels=n_input_channels,\n",
    "                                      out_channels=n_output_channels,\n",
    "                                      kernel_size=conv_kernel_size)\n",
    "\n",
    "        \n",
    "        # decoder\n",
    "        self.decoder_linear1 = nn.Linear(ae_input_dim, ae_input_dim)\n",
    "        self.decoder_linear2 = nn.Linear(ae_input_dim, ae_input_dim)\n",
    "\n",
    "    def encoder(self, x):\n",
    "        x = self.encoder_linear1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.encoder_linear2(x)\n",
    "        x = F.relu(x)\n",
    "        return x\n",
    "\n",
    "    def decoder(self, x):\n",
    "        x = self.decoder_linear1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.decoder_linear2(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "         # Apply hidden layers module\n",
    "        x = self.hidden_layers(x)\n",
    "\n",
    "        # Apply last layer (=output layer)\n",
    "        x = self.output_layer(x)\n",
    "        x = x.view(-1,1,90*90)\n",
    "        x = self.decoder(x)\n",
    "        x = x.view(-1,1,90,90)\n",
    "        return x\n",
    "\n",
    "net = AE_CNN()\n",
    "torch.save(net, os.path.join(config[\"results_path\"], \"train.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AE_CNN(\n",
       "  (hidden_layers): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (5): ReLU()\n",
       "    (6): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (9): ReLU()\n",
       "    (10): ConvTranspose2d(32, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (11): ReLU()\n",
       "    (12): ConvTranspose2d(32, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (13): ReLU()\n",
       "    (14): ConvTranspose2d(32, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (15): ReLU()\n",
       "    (16): ConvTranspose2d(32, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (17): ReLU()\n",
       "    (18): ConvTranspose2d(32, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (19): ReLU()\n",
       "    (20): ConvTranspose2d(32, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (21): ReLU()\n",
       "  )\n",
       "  (output_layer): Conv2d(32, 1, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (decoder_linear1): Linear(in_features=8100, out_features=8100, bias=True)\n",
       "  (decoder_linear2): Linear(in_features=8100, out_features=8100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "net = AE_CNN()\n",
    "torch.save(net, os.path.join(config[\"results_path\"], \"AE_CNN_model1.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 415
    },
    "executionInfo": {
     "elapsed": 329701,
     "status": "error",
     "timestamp": 1629797771025,
     "user": {
      "displayName": "Fabio PÃ¶schko",
      "photoUrl": "",
      "userId": "12121821188507889824"
     },
     "user_tz": -120
    },
    "id": "MbPtIOqzLK4u",
    "outputId": "3aed9de1-95c0-4e9f-85d4-a21127d7c6b4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuing the training:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| sum_loss: tensor(1.1460)\n",
      "  1%|          | 1/100 [02:09<3:34:21, 129.92s/it]ic| sum_loss: tensor(1.1459)\n",
      "  2%|â         | 2/100 [04:17<3:29:28, 128.25s/it]ic| sum_loss: tensor(1.1458)\n",
      "  3%|â         | 3/100 [07:45<4:10:39, 155.05s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-2aadb12168e2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[0minput_array\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0moriginal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[1;31m# ic(input_array.unsqueeze(1).type(torch.float32).shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_array\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[1;31m# Compute the loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\programing_in_python_2\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-fd610d595f7c>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m          \u001b[1;31m# Apply hidden layers module\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_layers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[1;31m# Apply last layer (=output layer)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\programing_in_python_2\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\programing_in_python_2\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\programing_in_python_2\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\programing_in_python_2\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, output_size)\u001b[0m\n\u001b[0;32m    914\u001b[0m             input, output_size, self.stride, self.padding, self.kernel_size, self.dilation)  # type: ignore[arg-type]\n\u001b[0;32m    915\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 916\u001b[1;33m         return F.conv_transpose2d(\n\u001b[0m\u001b[0;32m    917\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    918\u001b[0m             output_padding, self.groups, self.dilation)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create an instance of our NET\n",
    "net = torch.load(os.path.join(config[\"results_path\"], \"train.pt\"))\n",
    "\n",
    "# GPU will be much faster here\n",
    "device = torch.device(config[\"device\"])\n",
    "net.to(device=device)\n",
    "\n",
    "# also try rmse and absolute error\n",
    "loss_function = torch.nn.MSELoss(reduction=\"mean\")\n",
    "\n",
    "# Use a SGD optimizer\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=1e-2)\n",
    "#optimizer = torch.optim.Adam(net.parameters(), lr=config[\"learningrate\"])\n",
    "\n",
    "# Define a tensorboard summary writer that writes to directory \"results_path/tensorboard\"\n",
    "writer = SummaryWriter(log_dir=os.path.join(config[\"results_path\"], 'tensorboard'), flush_secs=1)\n",
    "\n",
    "best_loss = np.float(\"inf\")\n",
    "# Optimize our dsnn model using SGD:\n",
    "print(\"Continuing the training:\")\n",
    "\n",
    "# for update in tqdm(range(200)):\n",
    "with tqdm(total=config[\"n_updates\"]) as pbar:\n",
    "    for update in range(config[\"n_updates\"]):\n",
    "        x = 0\n",
    "        for data in training_loader:\n",
    "            # Compute the output\n",
    "            input_array,  original = data\n",
    "            # ic(input_array.unsqueeze(1).type(torch.float32).shape)\n",
    "            output = net(input_array.unsqueeze(1).type(torch.float32))\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = loss_function(output, original.type(torch.float32).unsqueeze(1))\n",
    "            # Compute the gradients\n",
    "            loss.backward()\n",
    "            # Preform the update\n",
    "            optimizer.step()\n",
    "            # Reset the accumulated gradients\n",
    "            optimizer.zero_grad()\n",
    "            x += 1\n",
    "            if x == 100:\n",
    "                break\n",
    "\n",
    "        # automatic saving of the best model\n",
    "        if update % 1 == 0:\n",
    "            with torch.no_grad():\n",
    "                y = 0\n",
    "                sum_loss = 0\n",
    "                for tdata in test_loader:\n",
    "                    input_array, original = data\n",
    "                    output = net(input_array.unsqueeze(1).type(torch.float32))\n",
    "                    sum_loss += loss_function(output, original.type(torch.float32).unsqueeze(1))\n",
    "                    y += 1\n",
    "                    if y == 10:\n",
    "                        ic(sum_loss)\n",
    "                        break\n",
    "                    \n",
    "            #if sum_loss < best_loss:\n",
    "            #  #  torch.save(net, os.path.join(config[\"results_path\"], \"autosave\", f\"loss_{sum_loss}.pt\"))\n",
    "            #    best_loss = loss\n",
    "\n",
    "        # Tensorboard\n",
    "        #if update % 1 == 0:\n",
    "        #    # Add losse as scalars to tensorboard\n",
    "        #    writer.add_scalar(tag=\"training/loss\", scalar_value=sum_loss,\n",
    "        #                      global_step=update)\n",
    "#\n",
    "        #    # Add images to Tensorboard\n",
    "        #    writer.add_image(tag=\"training/output\", img_tensor=output[0], global_step=update)\n",
    "#\n",
    "        #    writer.add_image(tag=\"training/input\", img_tensor=(original.unsqueeze(1)[0]), global_step=update)\n",
    "\n",
    "        torch.save(net, os.path.join(config[\"results_path\"], \"train.pt\"))\n",
    "        pbar.update(1)\n",
    "\n",
    "print(\"training finished\")\n",
    "#torch.save(net, os.path.join(config[\"results_path\"], \"my_model_A.pt\"))\n",
    "print(\"model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "110 sec with cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Td7u-scYLYRg"
   },
   "source": [
    "note to myself:\n",
    "activate tensorboard like this:\n",
    "\n",
    "activate programing_in_python_2\n",
    "tensorboard --logdir=C:\\Users\\fabio\\Google_Drive\\AI_Studium\\programing_python\\ass2\\ex5\\results\\tensorboard\n",
    "\n",
    "http://localhost:6006/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TZi8BOmhHQyp"
   },
   "outputs": [],
   "source": [
    "# trying another structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "TsND93NMHQyq"
   },
   "outputs": [],
   "source": [
    "from architectures import *\n",
    "from dataset_numpy import *\n",
    "from utils import *\n",
    "from torch.utils.data import Subset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import json\n",
    "import h5py\n",
    "from icecream import ic\n",
    "import numpy as np\n",
    "\n",
    "#getting the configurations from the config file\n",
    "with open('working_config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "    \n",
    "testset = RandomSeqDataset(r\"D:\\studium_tests\\small_original_dataset2.npz\",config[\"testset_key\"])\n",
    "trainset = RandomSeqDataset(r\"D:\\studium_tests\\small_original_dataset2.npz\",config[\"trainset_key\"])\n",
    "\n",
    "\n",
    "# Create dataloaders from each subset\n",
    "test_loader = DataLoader(testset,  # we want to load our dataset\n",
    "                         shuffle=False,  # shuffle for training\n",
    "                         batch_size=1,  # 1 sample at a time\n",
    "                         num_workers=0,\n",
    "                         collate_fn=stack_if_possible_collate_fn  # no background workers\n",
    "                         )\n",
    "\n",
    "training_loader = DataLoader(trainset,  # we want to load our dataset\n",
    "                             shuffle=False,  # shuffle for training\n",
    "                             batch_size=4,  # stack 4 samples to a minibatch\n",
    "                             num_workers=0,\n",
    "                             collate_fn=stack_if_possible_collate_fn  # 2 background workers\n",
    "                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AE_CNN(nn.Module):\n",
    "    def __init__(self, ae_input_dim = 90*90, ae_middle_dim = 80*80, ae_hidden_dim = 70*70,\n",
    "                n_input_channels = 1, n_output_channels = 1, conv_kernel_size = 5, n_hidden_layers=5, n_hidden_kernels=32):\n",
    "        super(AE_CNN, self).__init__()\n",
    "        # encoder\n",
    "        #self.encoder_linear1 = nn.Linear(ae_input_dim, ae_input_dim)\n",
    "        #self.encoder_linear2 = nn.Linear(ae_input_dim, ae_input_dim)\n",
    "        \n",
    "        # convolution\n",
    "        hidden_layers = []\n",
    "        for _ in range(n_hidden_layers):\n",
    "            # Add a CNN layer\n",
    "            layer = nn.Conv2d(in_channels=n_input_channels,\n",
    "                              out_channels=n_hidden_kernels,\n",
    "                              kernel_size=conv_kernel_size)\n",
    "            hidden_layers.append(layer)\n",
    "            # Add relu activation module to list of modules\n",
    "            hidden_layers.append(nn.ReLU())\n",
    "            n_input_channels = n_hidden_kernels\n",
    "\n",
    "            \n",
    "        self.hidden_layers = nn.Sequential(*hidden_layers)\n",
    "\n",
    "        self.output_layer = nn.Conv2d(in_channels=n_input_channels,\n",
    "                                      out_channels=n_output_channels,\n",
    "                                      kernel_size=conv_kernel_size)\n",
    "\n",
    "        \n",
    "        # decoder\n",
    "        self.decoder_linear1 = nn.Linear(66*66, 80*80)\n",
    "        self.decoder_linear2 = nn.Linear(80*80, ae_input_dim)\n",
    "\n",
    "    #def encoder(self, x):\n",
    "    #    x = self.encoder_linear1(x)\n",
    "    #    x = F.relu(x)\n",
    "    #    x = self.encoder_linear2(x)\n",
    "    #    x = F.relu(x)\n",
    "    #    return x\n",
    "\n",
    "    def decoder(self, x):\n",
    "        x = self.decoder_linear1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.decoder_linear2(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "         # Apply hidden layers module\n",
    "        x = self.hidden_layers(x)\n",
    "\n",
    "        # Apply last layer (=output layer)\n",
    "        x = self.output_layer(x)\n",
    "        x = x.view(-1,1,66*66)\n",
    "        x = self.decoder(x)\n",
    "        x = x.view(-1,1,90,90)\n",
    "        return x\n",
    "\n",
    "net = AE_CNN()\n",
    "#torch.save(net, os.path.join(config[\"results_path\"], \"without_deconvolution.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, n_input_channels: int, n_hidden_layers: int,\n",
    "                 n_hidden_kernels: int, n_output_channels: int):\n",
    "        \"\"\"CNN, consisting of `n_hidden_layers` linear layers, using relu\n",
    "        activation function in the hidden CNN layers.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_input_channels: int\n",
    "            Number of features channels in input tensor\n",
    "        n_hidden_layers: int\n",
    "            Number of hidden layers\n",
    "        n_hidden_kernels: int\n",
    "            Number of kernels in each hidden layer\n",
    "        n_output_channels: int\n",
    "            Number of features in output tensor\n",
    "        \"\"\"\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        # encoder\n",
    "        hidden_layers = []\n",
    "        for _ in range(n_hidden_layers):\n",
    "            # Add a CNN layer\n",
    "            layer = nn.Conv2d(in_channels=n_input_channels,\n",
    "                              out_channels=n_hidden_kernels,\n",
    "                              kernel_size=7)\n",
    "            hidden_layers.append(layer)\n",
    "            # Add relu activation module to list of modules\n",
    "            hidden_layers.append(nn.ReLU())\n",
    "            n_input_channels = n_hidden_kernels\n",
    "\n",
    "        # decoder\n",
    "        for _ in range(n_hidden_layers + 1):\n",
    "            # Add a CNN layer\n",
    "            layer = nn.ConvTranspose2d(in_channels=n_input_channels,\n",
    "                                       out_channels=n_hidden_kernels,\n",
    "                                       kernel_size=7)\n",
    "            hidden_layers.append(layer)\n",
    "            # Add relu activation module to list of modules\n",
    "            hidden_layers.append(nn.ReLU())\n",
    "            n_input_channels = n_hidden_kernels\n",
    "\n",
    "        self.hidden_layers = nn.Sequential(*hidden_layers)\n",
    "\n",
    "        self.output_layer = nn.Conv2d(in_channels=n_input_channels,\n",
    "                                      out_channels=n_output_channels,\n",
    "                                      kernel_size=7)\n",
    "        self.linear1 = nn.Linear(90*90, 90*90)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Apply CNN to `x`\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: torch.tensor\n",
    "            Input tensor of shape (n_samples, n_input_channels, x, y)\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        torch.tensor\n",
    "            Output tensor of shape (n_samples, n_output_channels, u, v)\n",
    "        \"\"\"\n",
    "        # Apply hidden layers module\n",
    "        hidden_features = self.hidden_layers(x)\n",
    "\n",
    "        # Apply last layer (=output layer)\n",
    "        output = self.output_layer(hidden_features)\n",
    "        output = x.view(-1,1,90*90)\n",
    "        output = self.linear1(output)\n",
    "        output = torch.sigmoid(output)\n",
    "        output = output.view(-1,1,90,90)\n",
    "        return output\n",
    "\n",
    "net = CNN(n_input_channels=config[\"network_config\"][\"n_input_channels\"], n_hidden_layers=config[\"network_config\"][\"n_hidden_layers\"], n_hidden_kernels=config[\"network_config\"][\"n_hidden_kernels\"],\n",
    "          n_output_channels=config[\"network_config\"][\"n_output_channels\"])\n",
    "torch.save(net, os.path.join(config[\"results_path\"], \"new_cnn.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (hidden_layers): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(7, 7), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 32, kernel_size=(7, 7), stride=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(32, 32, kernel_size=(7, 7), stride=(1, 1))\n",
       "    (5): ReLU()\n",
       "    (6): ConvTranspose2d(32, 32, kernel_size=(7, 7), stride=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): ConvTranspose2d(32, 32, kernel_size=(7, 7), stride=(1, 1))\n",
       "    (9): ReLU()\n",
       "    (10): ConvTranspose2d(32, 32, kernel_size=(7, 7), stride=(1, 1))\n",
       "    (11): ReLU()\n",
       "    (12): ConvTranspose2d(32, 32, kernel_size=(7, 7), stride=(1, 1))\n",
       "    (13): ReLU()\n",
       "  )\n",
       "  (output_layer): Conv2d(32, 1, kernel_size=(7, 7), stride=(1, 1))\n",
       "  (linear1): Linear(in_features=8100, out_features=8100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuing the training:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| sum_loss: tensor(4.6243)\n",
      "  1%|          | 1/100 [09:09<15:06:25, 549.35s/it]"
     ]
    }
   ],
   "source": [
    "#claring some folders\n",
    "for key in config[\"to_clear\"]:\n",
    "    clear_folder(config[\"to_clear\"][key])\n",
    "\n",
    "# Create an instance of our NET\n",
    "net = torch.load(os.path.join(config[\"results_path\"], \"new_cnn.pt\"))\n",
    "\n",
    "# GPU will be much faster here\n",
    "device = torch.device(config[\"device\"])\n",
    "net.to(device=device)\n",
    "\n",
    "# also try rmse and absolute error\n",
    "loss_function = torch.nn.MSELoss(reduction=\"mean\")\n",
    "\n",
    "# Use a SGD optimizer\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=1e-5)\n",
    "#optimizer = torch.optim.SGD(net.parameters(), lr=1e+1)\n",
    "#optimizer = torch.optim.Adam(net.parameters(), lr=config[\"learningrate\"])\n",
    "\n",
    "# Define a tensorboard summary writer that writes to directory \"results_path/tensorboard\"\n",
    "writer = SummaryWriter(log_dir=os.path.join(config[\"results_path\"], 'tensorboard'), flush_secs=1)\n",
    "\n",
    "best_loss = np.float(\"inf\")\n",
    "# Optimize our dsnn model using SGD:\n",
    "print(\"Continuing the training:\")\n",
    "\n",
    "# for update in tqdm(range(200)):\n",
    "with tqdm(total=config[\"n_updates\"]) as pbar:\n",
    "    for update in range(config[\"n_updates\"]):\n",
    "        x = 0\n",
    "        for data in training_loader:\n",
    "            # Compute the output\n",
    "            input_array,  original = data\n",
    "            # ic(input_array.unsqueeze(1).type(torch.float32).shape)\n",
    "            output = net(input_array.unsqueeze(1).type(torch.float32))\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = loss_function(output, original.type(torch.float32).unsqueeze(1))\n",
    "            # Compute the gradients\n",
    "            loss.backward()\n",
    "            # Preform the update\n",
    "            optimizer.step()\n",
    "            # Reset the accumulated gradients\n",
    "            optimizer.zero_grad()\n",
    "            x += 1\n",
    "            if x == 1000:\n",
    "                break\n",
    "\n",
    "        # automatic saving of the best model\n",
    "        if update % 1 == 0:\n",
    "            with torch.no_grad():\n",
    "                y = 0\n",
    "                sum_loss = 0\n",
    "                for tdata in test_loader:\n",
    "                    input_array, original = data\n",
    "                    output = net(input_array.unsqueeze(1).type(torch.float32))\n",
    "                    sum_loss += loss_function(output, original.type(torch.float32).unsqueeze(1))\n",
    "                    y += 1\n",
    "                    if y == 100:\n",
    "                        ic(sum_loss)\n",
    "                        break\n",
    "                    \n",
    "            if sum_loss < best_loss:\n",
    "                torch.save(net, os.path.join(config[\"results_path\"], \"autosave\", f\"loss_{sum_loss}.pt\"))\n",
    "                best_loss = loss\n",
    "\n",
    "        # Tensorboard\n",
    "        if update % 1 == 0:\n",
    "            # Add losse as scalars to tensorboard\n",
    "            writer.add_scalar(tag=\"training/loss\", scalar_value=sum_loss,\n",
    "                              global_step=update)\n",
    "\n",
    "            # Add images to Tensorboard\n",
    "            writer.add_image(tag=\"training/output\", img_tensor=output[2], global_step=update)\n",
    "\n",
    "            writer.add_image(tag=\"training/input\", img_tensor=(original.unsqueeze(1)[2]), global_step=update)\n",
    "\n",
    "        torch.save(net, os.path.join(config[\"results_path\"], \"new_cnn.pt\"))\n",
    "        pbar.update(1)\n",
    "\n",
    "print(\"training finished\")\n",
    "#torch.save(net, os.path.join(config[\"results_path\"], \"my_model_A.pt\"))\n",
    "print(\"model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AE_CNN(\n",
       "  (encoder_linear1): Linear(in_features=8100, out_features=8100, bias=True)\n",
       "  (encoder_linear2): Linear(in_features=8100, out_features=8100, bias=True)\n",
       "  (hidden_layers): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (5): ReLU()\n",
       "    (6): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (9): ReLU()\n",
       "  )\n",
       "  (output_layer): Conv2d(32, 1, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (decoder_linear1): Linear(in_features=4356, out_features=8100, bias=True)\n",
       "  (decoder_linear2): Linear(in_features=8100, out_features=8100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "142.46833333333333"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_loader)*11/10/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in training_loader:\n",
    "    input_array,  original = data\n",
    "    output = net(input_array.unsqueeze(1).type(torch.float32))\n",
    "    break\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def array_to_img(image):\n",
    "    img = Image.fromarray(image,)\n",
    "    img.show()\n",
    "\n",
    "array_to_img(original[2].detach().numpy()*255)\n",
    "array_to_img(input_array[2].detach().numpy()*255)\n",
    "array_to_img(output[2][0].detach().numpy()*255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 90, 90])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "colab_final_predictions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
